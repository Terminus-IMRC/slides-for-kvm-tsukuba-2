/自己紹介
 - 杉崎 行優
 - 並木中等教育学校5年次
   - 課題をやらない -> 単位が危ない
 - Makefileガチ勢
 - その他の情報は http://imrc.noip.me/ へ
 - Twitter: @n_IMRC
 - Yo: IMRC
 - Github: Terminus-IMRC

/USBキーボード to Bluetooth コンバータの製作
 - HHKB用の(?)売ってるけど高い
 - まずリレーで失敗
 - 秋月でRN-42を発見
 - プログラム書いて、できた
   - Serial.printに渡す値はキャストしよう

/RaspberryPi便利化
 - サーバとして使っている
 - 自分でinitを書いてハードディスクからブート
 - thd

/framebuffer2imageとmjpg_streamerによるコンソール画面のストリーミング

/XeonPhiがスケールしない
 - 並列計算
   - 複数のコンピュータを接続し、分散処理
   - 全体の実行時間を短縮
     - 理想は (元の実行時間)/n
     - 通信やアルゴリズムのオーバヘッド等により、現実的に不可能
   - 数値計算においてはMPI, OpenMP, pthread等の高レベルなライブラリが使用される
 - スーパーコンピュータ
   - コンピュータ(多少高性能)の集合体
     - コンピュータが並列計算をする
   - 平成26年度 政府調達手続に関する運用指針 別紙2「50TFLOPS以上の理論的最高性能を有するスーパーコンピュータ...」
 - 筑波大学学際共同利用
   - 申請が通れば筑波大学のスパコンを1年間利用可能
   - 年2回の報告会参加義務あり
   - 5TBのストレージも利用可能
   - 活発には利用されていない気がする
   - (私でも審査通ったし。。。)
 - 筑波大学のスパコン環境
   - T2K-Tsukuba
     - 1ノード: quad-core AMD Opteron x 4
     - ネットワークは 4X DDR Infiniband (16Gbit/s)
     - 全体で648ノード
     - 95.4TFLOPSの理論ピーク性能
   - HA-PACS
     - 1ノード: SandyBrigde 8cores x 2, NVIDIA M2090(Fermi) x 4
     - ネットワークは 4X QDR Infiniband (32Gbits/s)
     - 全体で268ノード
     - 802TFLOPSの理論ピーク性能
   - COMA (PACS-IX)
     - 1ノード: SandyBridge 8cores x 2, XeonPhi 61cores x 2
     - ネットワークは 4x FDR Inifiniband (56GBits/s)
     - 全体で393ノード
     - 1PFLOPSの理論ピーク性能
   - ソフトウェア的な計算環境
     - ノード間通信はMPI
     - ノード内通信はOpenMPやpthreadやMPI
     - CPU間通信にNUMAを使うこともある
     - ユーザがジョブを投げ、スケジューラが適当な計算ノードにそれを割り当て、実行
 - 昨年度の申請/研究内容
   - n次魔方陣の全ての解を求めるプログラムの並列化と性能測定
     - T2K-Tsukuba
     - 魔方陣とは
     - embarrassingly parallelな問題...用意に並列化可能
     - マスタ/ワーカー方式...1台のマスタから配布された問題を残りのワーカーが計算
     - 通信の頻度が高すぎたり、配布する問題が粗いと、性能低下
     - -> 適切な通信頻度/問題粒度の設定が必要 <- 当たり前だろ
 - 今年度の申請/研究内容
   - n次魔方陣の全ての解を求めるプログラムのメニーコア化
     - COMA
     - 現実は厳しい
       - 性能が出ない
       - スケールしない
       - スケールしない
       - スケールしない
     - COMAった
 - 1-0: laplace方程式のシミュレーション
   - とは
   - フィールドの値を更新していき、値が更新されなくなったら終了
   - 1ターンあたりの計算量が比較的少ない
   - まずは1ノードで実行(マルチスレッドのみ)
   - フィールドの大きさは5000x5000とした
 - 1-1: pthreadでassociative fan-in algorithmを用いたgather
   - 気合を入れてOpenMPではなくpthread (後悔)
   - associative fan-in algorithm...log2(N)段階のgather
   - XeonPhiではpthread_barrier_waitの時間が支配的
   - Xeonではpthread_barrier_waitの時間が支配的
   - (Xeonの20スレッド以上で伸び悩んでるのはキャッシュの問題...?)
 - 1-2-1: pthreadで1段階のgather
   - 1スレッドが他の全スレッドの更新状態を読みにいく
   - 1-1と比較して、どちらも実行時間が短くなった
   - XeonPhiはスレッド数の増加に従って性能低下
   - Xeonもスレッド数の増加に従って性能低下
   - スレッド数が増えると全スレッドの状態を読みにいくのがつらそう
 - 1-2-2: pthreadで1段階のgather
   - 30ターンに1度、1スレッドが他の全スレッドの更新状態を読みにいく

